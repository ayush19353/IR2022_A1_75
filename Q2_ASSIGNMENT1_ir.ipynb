{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2_ASSIGNMENT1_ir.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import csv\n",
        "import cv2\n",
        "from google.colab import files\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install ipython-autotime\n",
        "from natsort import natsorted\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "F4luVntt1lO6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56aaa2e-5ef4-49e2-e228-9a9c0933f958"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pSY3OTGPKKc",
        "outputId": "6f149df3-2bcc-4f37-f734-3955698bd646"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "reDJfjeL2D9L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "alPVJsjb2KYV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "    with open(filename, 'r', encoding =\"ascii\", errors =\"surrogateescape\") as f:\n",
        "        s = f.read()\n",
        "    f.close()\n",
        "    \n",
        "    return s.lower()\n",
        "def remove(string_a):\n",
        "    string_fin= \"\"\n",
        "    tokens =string_a.split('\\n\\n')\n",
        "    for token in tokens[0:]:\n",
        "        string_fin=string_fin+ token+\" \"\n",
        "    return string_fin.lower() "
      ],
      "metadata": {
        "id": "eJexttEH2Kya"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(final_token):\n",
        "    trans_tab= str.maketrans('', '', '\\t')# Removing punctuations. # Q2 part a part iv)\n",
        "    final_token = [word1.translate(trans_tab) for word1 in final_token]\n",
        "    name_punc = (string.punctuation).replace(\"'\", \"\")\n",
        "    trans_table = str.maketrans('', '', name_punc)\n",
        "    wordss= [word.translate(trans_table) for word in final_token]\n",
        "    final_token = [str for str in wordss if str]\n",
        "    return final_token"
      ],
      "metadata": {
        "id": "W_R0I1Fm2Qit"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_Ket-0ch2RKu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "path = \"/content/drive/MyDrive/Humor,Hist,Media,Food\"\n",
        "os.chdir(path)\n",
        "names_file = natsorted(os.listdir(path))\n",
        "\n",
        "def preprocessing(string_b):\n",
        "    tokenizer = TweetTokenizer() # Q2 part a part ii)\n",
        "    final_token = tokenizer.tokenize(string_b)\n",
        "    stoping_word= set(stopwords.words(\"english\")) # stop words \n",
        "    final_token = [word1 for word1 in final_token if word1 not in stoping_word]# removing stop words # Q2 part a part iii)\n",
        "    \n",
        "    final_token= remove_punc(final_token) # reomving punctiations\n",
        " \n",
        "    final_token =[word1.lower() for word1 in final_token] #Change to lower_Case # Q2 part a part i)\n",
        " \n",
        "    while(\"\" in final_token) :# Remoibing blank spaces. # Q2 part a part v)\n",
        "      final_token.remove(\"\")\n",
        "    while(\" \" in final_token) :\n",
        "      final_token.remove(\" \")\n",
        "    return final_token"
      ],
      "metadata": {
        "id": "biny3fm72U3z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_position(t,positions_1, file_number,positions1):\n",
        "              positions_1[t] = []\n",
        "              positions_1[t].append(1)\n",
        "              positions_1[t].append({})  \n",
        "              return  [positions1]"
      ],
      "metadata": {
        "id": "77dsAzbgdG5-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# refered from GFG https://www.geeksforgeeks.org/python-positional-index/ \n",
        "file_number = 0\n",
        "fileName=[]\n",
        "positions_1= {} # dictionary to store the frequencies and the psoitions of the tokens in tthe documents \n",
        "for file_name in names_file:\n",
        "        fileName.append(file_name)\n",
        "        s= read_file(path+'/' + file_name)\n",
        "        try:\n",
        "          tokenS_list = preprocessing(s)\n",
        "\n",
        "          for positions1, t in enumerate(tokenS_list):\n",
        "            if t not  in positions_1:\n",
        "              positions_1[t][1][file_number] = return_position(t,positions_1, file_number,positions1)\n",
        "            else:\n",
        "\n",
        "              positions_1[t][0]=1+ positions_1[t][0]\n",
        "              if file_number not in positions_1[t][1]:\n",
        "                positions_1[t][1][file_number]=[positions1]\n",
        "              else:\n",
        "                positions_1[t][1][file_number].append(positions1)\n",
        " \n",
        "        except:\n",
        "          print(file_number)         \n",
        "        file_number += 1\n"
      ],
      "metadata": {
        "id": "viJCNeRP2ZEv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OF7hJyO76n6L"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z2kzEWI1WznY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cv27NggL2de5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5r2YzMH88nUg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_append(a1,l1,t_c,k1,tot_docc):\n",
        "        if l1==t_c:\n",
        "                return (a1[k1][0])\n",
        "def return_doc( az):\n",
        "  docs_list = [z1 for z1 in az]\n",
        "  return docs_list               "
      ],
      "metadata": {
        "id": "WkgO6lJw-Frt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gM4oDRG1-HTO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_word(q_token,query):\n",
        "        print(\"total documents\")\n",
        "        c1=0\n",
        "        zzzz=[]\n",
        "        if(query not in positions_1.keys()):\n",
        "          print(\"EMPTY\")\n",
        "          return\n",
        "\n",
        "        for a in positions_1[query][1]:\n",
        "            print(a)\n",
        "            zzzz.append(fileName[a])\n",
        "            c1=c1+1\n",
        "        print(zzzz)\n",
        "        print(c1)\n",
        "        \n",
        "        return\n",
        "def return_to_delete(q_token):\n",
        "    \n",
        "    return q_token[0]"
      ],
      "metadata": {
        "id": "A3DdTnzfbX1k"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answers(query): # queery \n",
        "    q_token =(preprocessing(query)) # prpcessed aeury returning tokens\n",
        "   # print()\n",
        "   # print(\"DDDD\")\n",
        "    \n",
        "    print(q_token)\n",
        "    if(len(q_token)==0):\n",
        "      print(\"EMPTY\")\n",
        "\n",
        "      return\n",
        "\n",
        "    #print(\"WEE\")\n",
        "    print(type(q_token))\n",
        "    if len(q_token)==1: # singel word\n",
        "        single_word(q_token,query)\n",
        "        return\n",
        "    fin_mod=[]\n",
        "    if(q_token[0] not in positions_1.keys()):\n",
        "      print(\"EMPTY\")\n",
        "      return\n",
        "    word_postings = positions_1[q_token[0]][1]\n",
        "    for ps in positions_1[q_token[0]][1]:\n",
        "        for p in positions_1[q_token[0]][1][ps]:\n",
        "            fin_mod.append((ps, p))\n",
        "    \n",
        "    del q_token[0]\n",
        "    tot_count_docs_match=[] # documetn numberss\n",
        "    \n",
        "    tot_count_docs_match1=[] # document names\n",
        "    k1=0\n",
        "    while k1< len(fin_mod): # multiple words in query\n",
        "        zz= fin_mod[k1]\n",
        "        t_c=0\n",
        "        pos=zz[1]\n",
        "        for k in q_token:#For the dictionary of the first token in form of  { doc id, position array}, iterate over all the doc IDs of the first token, and for all positions in that list as start position index and see for the subsequent query tokens. That very doc ID would be the keys,and the next start as the starting postion.\n",
        "\n",
        "            if(k not in positions_1.keys()):\n",
        "               print(\"EMPTY\")\n",
        "               return\n",
        "            pos=pos+1\n",
        "            if zz[0] in return_doc(positions_1[k][1]):\n",
        "                doc_positions ={}\n",
        "                for posting_value in positions_1[k][1]:\n",
        "        \n",
        "                  if posting_value == zz[0]:\n",
        "                    doc_positions= positions_1[k][1][posting_value]\n",
        "                    break\n",
        "                \n",
        "                \n",
        "                if  pos not in doc_positions:\n",
        "                    t_c=1+t_c\n",
        "                    break\n",
        "                else:\n",
        "                    t_c=1+t_c\n",
        "            if t_c== len(q_token):\n",
        "                zza=return_append(fin_mod,len(q_token),t_c,k1,tot_count_docs_match)\n",
        "                tot_count_docs_match.append(zza)\n",
        "                tot_count_docs_match1.append(fileName[zza])\n",
        "        k1=k1+1    #With these conditions the docid is added to the lsit.\n",
        "\n",
        "    print(\"Total Document Matches:\", set(tot_count_docs_match))\n",
        "    print(\"NAMES  \", set(tot_count_docs_match1))\n",
        "    print(len(set(tot_count_docs_match)))\n",
        "    "
      ],
      "metadata": {
        "id": "DaVxOhoz-Ir6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_a='welcome'\n",
        "\n",
        "token_a= preprocessing(token_a)\n",
        "   # print(preprocessed_word[0])\n",
        "print(\"Frequency:\",positions_1[token_a[0]][0])\n",
        "print(\"Postings List:\",positions_1[token_a[0]][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDITJJcE-Lny",
        "outputId": "7a1b3377-9ff3-4881-8223-bbed54cccf98"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency: 148\n",
            "Postings List: {5: [410], 13: [103], 32: [326, 798, 2524], 89: [180], 91: [941], 106: [31], 109: [89], 127: [64], 129: [1429, 2165], 130: [1318, 2020], 135: [325, 327], 145: [4146], 167: [78], 184: [23682], 249: [1929], 250: [1170], 268: [2029], 278: [123], 284: [1191], 286: [860, 3118], 287: [25, 6117], 298: [2, 79, 83], 310: [13, 89], 315: [2931], 324: [1121], 339: [973], 345: [1288, 2001], 348: [3918], 349: [154], 373: [290], 380: [4485], 426: [1661], 428: [304], 434: [137], 441: [1529, 2409, 2413], 451: [957, 1112], 479: [442, 1472], 484: [78], 488: [99, 373], 501: [17851], 519: [4680, 7014], 520: [361, 6844], 530: [463, 626], 548: [1281, 1460], 574: [12, 356], 576: [516], 585: [102, 132], 598: [25, 287, 569], 604: [93, 388], 607: [333], 617: [922, 3180], 618: [105, 354, 555, 614, 900], 626: [3], 630: [1522], 636: [386, 536], 652: [398], 655: [2025, 2456], 659: [25], 663: [1027, 8602, 18077], 665: [11], 672: [1930], 692: [135, 196], 711: [148], 724: [88], 730: [210], 745: [448], 748: [233], 749: [224], 752: [3, 2770], 753: [3], 771: [606, 921], 816: [1249], 835: [0, 476, 490, 1160], 846: [1894, 7378], 853: [150, 1091, 1127], 893: [38], 949: [110], 956: [113, 936, 1407, 1455], 968: [13, 25], 981: [41, 4908], 982: [31, 205], 997: [242], 1017: [1133], 1018: [1133], 1027: [450, 601, 1668], 1034: [0, 79], 1035: [1253], 1036: [4341], 1046: [1480], 1054: [193, 613], 1063: [314], 1070: [7266], 1097: [287], 1120: [1595, 4510]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"welcome\"\n",
        "lists = answers(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1V6yS_K-ajz",
        "outputId": "0157b494-152c-4d91-dce8-8c317f3b1b3f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome']\n",
            "<class 'list'>\n",
            "total documents\n",
            "5\n",
            "13\n",
            "32\n",
            "89\n",
            "91\n",
            "106\n",
            "109\n",
            "127\n",
            "129\n",
            "130\n",
            "135\n",
            "145\n",
            "167\n",
            "184\n",
            "249\n",
            "250\n",
            "268\n",
            "278\n",
            "284\n",
            "286\n",
            "287\n",
            "298\n",
            "310\n",
            "315\n",
            "324\n",
            "339\n",
            "345\n",
            "348\n",
            "349\n",
            "373\n",
            "380\n",
            "426\n",
            "428\n",
            "434\n",
            "441\n",
            "451\n",
            "479\n",
            "484\n",
            "488\n",
            "501\n",
            "519\n",
            "520\n",
            "530\n",
            "548\n",
            "574\n",
            "576\n",
            "585\n",
            "598\n",
            "604\n",
            "607\n",
            "617\n",
            "618\n",
            "626\n",
            "630\n",
            "636\n",
            "652\n",
            "655\n",
            "659\n",
            "663\n",
            "665\n",
            "672\n",
            "692\n",
            "711\n",
            "724\n",
            "730\n",
            "745\n",
            "748\n",
            "749\n",
            "752\n",
            "753\n",
            "771\n",
            "816\n",
            "835\n",
            "846\n",
            "853\n",
            "893\n",
            "949\n",
            "956\n",
            "968\n",
            "981\n",
            "982\n",
            "997\n",
            "1017\n",
            "1018\n",
            "1027\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1046\n",
            "1054\n",
            "1063\n",
            "1070\n",
            "1097\n",
            "1120\n",
            "['aboutada.txt', 'adcopy.hum', 'amazing.epi', 'bbq.txt', 'beauty.tm', 'beginn.ers', 'bhb.ill', 'bmdn01.txt', 'bnbeg2.4.txt', 'bnbguide.txt', 'boneles2.txt', 'bread.rcp', 'butcher.txt', 'candy.txt', 'comrevi1.hum', 'conan.txt', 'crzycred.lst', 'cybrtrsh.txt', 'dead2.txt', 'dead4.txt', 'dead5.txt', 'dieter.txt', 'dover.poem', 'drinks.gui', 'dym', 'english.txt', 'epi_bnb.txt', 'epi_tton.txt', 'epikarat.txt', 'feggaqui.txt', 'films_gl.txt', 'gd_tznew.txt', 'get.drunk.cheap', 'goforth.hum', 'grail.txt', 'hackmorality.txt', 'homebrew.txt', 'horflick.txt', 'hotel.txt', 'humor9.txt', 'insult.lst', 'insults1.txt', 'jac&tuu.hum', 'jokes1.txt', 'lawskool.txt', 'lawyer.jok', 'letter_f.sch', 'lipkovits.txt', 'looser.hum', 'lost.txt', 'luvstory.txt', 'luzerzo2.hum', 'making_y.wel', 'manners.txt', 'math.1', 'miami.hum', 'mindvox', 'misery.hum', 'mlverb.hum', 'modest.hum', 'moose.txt', 'myheart.hum', 'nigel.6', 'nukewar.txt', 'nysucks.hum', 'oldeng.hum', 'oliver02.txt', 'oliver.txt', 'onetoone.hum', 'onetotwo.hum', 'passage.hum', 'pracjoke.txt', 'psilaine.hum', 'quack26.txt', 'quotes.jok', 'renorthr.txt', 'snapple.rum', 'soleleer.hum', 'st_silic.txt', 'stuf10.txt', 'stuf11.txt', 'talebeat.hum', 'texican.dic', 'texican.lex', 'thievco.txt', 'top10.txt', 'top10st1.txt', 'top10st2.txt', 'truthlsd.hum', 'twilight.txt', 'urban.txt', 'vegan.rcp', 'wisconsi.txt', 'xibovac.txt']\n",
            "94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Strange names\"\n",
        "lists = answers(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIDD-VhTBAVC",
        "outputId": "11baaa5b-4551-4ed3-86af-8cc6c270c22a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['strange', 'names']\n",
            "<class 'list'>\n",
            "Total Document Matches: {4, 1092, 663}\n",
            "NAMES   {'abbott.txt', 'whoon1st.hum', 'mlverb.hum'}\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"bulletin board\"\n",
        "lists = answers(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiylW39XYBfy",
        "outputId": "08ee2ea5-226b-48c7-f1ca-a770987cee93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bulletin', 'board']\n",
            "<class 'list'>\n",
            "Total Document Matches: {771, 652, 908, 656, 278, 663, 280, 664, 540, 289, 929, 549, 678, 682, 1066, 428, 1067, 51, 437, 699, 828, 956, 449, 967, 456, 720, 209, 604, 221, 358, 491, 495}\n",
            "NAMES   {'apsnet.txt', 'mr.rogers', 'looser.hum', 'minn.txt', 'modemwld.txt', 'passage.hum', 'variety1.asc', 'how2bgod.txt', 'shrink.news', 'happyhack.txt', 'ripoffpc.hum', 'church.sto', 'hack7.txt', 'cybrtrsh.txt', 'msorrow', 'dalive', 'miami.hum', 'squids.gph', 'jokes.txt', 'exidy.txt', 'deadlysins.txt', 'chickenheadbbs.txt', 'mlverb.hum', 'get.drunk.cheap', 'netmask.txt', 'howlong.hum', 'jayjay.txt', 'novel.hum', 'proudlyserve.txt', 'soleleer.hum', 'golnar.txt', 'variety2.asc'}\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"it is a large mixer BOWL\"\n",
        "lists = answers(query)\n"
      ],
      "metadata": {
        "id": "VruhPHI4E_AX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e72010-15be-417f-e535-c34e355f06cd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['large', 'mixer', 'bowl']\n",
            "<class 'list'>\n",
            "Total Document Matches: {1090, 904, 329, 74, 340, 85}\n",
            "NAMES   {'engmuffn.txt', 'richbred.txt', 'batrbred.txt', 'whitbred.txt', 'egg-bred.txt', 'bakebred.txt'}\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileName[663]"
      ],
      "metadata": {
        "id": "8n7IUgnWF_Gd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1df21df9-0089-40c3-bd0b-313433fd06cf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mlverb.hum'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"xxdggdhdgk welcome\"\n",
        "\n",
        "lists = answers(query)"
      ],
      "metadata": {
        "id": "xje4RurO94sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e1ddfcb-c93a-4ef3-fa94-3bbce0807e31"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxdggdhdgk', 'welcome']\n",
            "<class 'list'>\n",
            "EMPTY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hUtOtKqaa721"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}