# -*- coding: utf-8 -*-
"""Q1_Assignment1_ir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xADS1K-LJ03GnH2PcDX6aKtkcI9x41lV
"""

from google.colab.patches import cv2_imshow
import csv
import cv2
from google.colab import files
!pip install nltk
!pip install pandas
!pip install numpy
!pip install ipython-autotime
from natsort import natsorted
import string
import matplotlib.pyplot as plt
import numpy as np
from google.colab.patches import cv2_imshow
import pandas as pd
from google.colab import files
import numpy as np
import os
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import nltk
from nltk.tokenize import RegexpTokenizer
from collections import defaultdict
import string
import re

# import these modules
from nltk.stem import WordNetLemmatizer
 


import nltk
nltk.download('stopwords')
nltk.download('punkt')
import nltk
nltk.download('wordnet')

nltk.download('stopwords')

# Mounting google drive to access the folder
from google.colab import drive
drive.mount('/content/drive')

# function to read the files
def read_file(filename):
    with open(filename,encoding= 'utf-8',errors='ignore') as f:
        stuff = f.read()
    f.close
    stuff = remove_header_footer(stuff) 
    return stuff
    
# function to convert numbers to strings
def convert_Numbers(final_string):
    final_string=re.sub(r'0',' zero ',final_string)
    final_string=re.sub(r'1',' one ',final_string)    
    final_string=re.sub(r'2',' two ',final_string)
    final_string=re.sub(r'3',' three ',final_string)
    final_string=re.sub(r'4',' four ',final_string)
    final_string=re.sub(r'5',' five ',final_string)
    final_string=re.sub(r'6',' six ',final_string)
    final_string=re.sub(r'7',' seven ',final_string)
    final_string=re.sub(r'8',' eight ',final_string)
    final_string=re.sub(r'9',' nine ',final_string)

    return final_string

# To remove apostrophe
def apost(final_string):
    final_string=re.sub(r"'","",final_string)
    return final_string

# function to remove punctuation
def remove_punc(final_token):
    trans_tab= str.maketrans('', '', '\t')# Removing punctuations. # Q2 part a part iv)
    final_token = [word1.translate(trans_tab) for word1 in final_token]
    name_punc = (string.punctuation).replace("'", "")
    trans_table = str.maketrans('', '', name_punc)
    wordss= [word.translate(trans_table) for word in final_token]
    final_token = [str for str in wordss if str]
    return final_token

# function to remove header and footer from the input sentence
def remove_header_footer(final_string):
    new_final_string = ""
    tokens = final_string.split('\n\n')
 
    # Remove tokens[0] and tokens[-1]
    for token in tokens[0: ]:
        new_final_string += token+" "
    return new_final_string.lower()  # first convert to lower


from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os

# defining the path and then storing the names of all the files in a list named as names_file
path = "/content/drive/MyDrive/Humor,Hist,Media,Food"
os.chdir(path)
names_file = natsorted(os.listdir(path))

# Function to perform complete preprocessing
def preprocessing(final_string):
  
    lemmatizer = WordNetLemmatizer() # lemmatization

    final_string = convert_Numbers(final_string) # converting numbers
    final_string= apost(final_string)
  #  final_string = remove_single_characters(final_string)
    #stemmer= PorterStemmer()     

 
    #final_string=re.sub(r'\d+','',final_string)#removing numbers
    tokenizer = TweetTokenizer()      # tokenisation
    token_l = tokenizer.tokenize(final_string)

    token_lz=[]
    for w in token_l:
        w=lemmatizer.lemmatize(w)
        token_lz.append(w)
    stop_words = set(stopwords.words("english")) # stop words 
    token_lz = [word for word in token_lz if word not in stop_words]# removing stop words
   # token_l= [np.char.replace(word, "'", "") for word in token_l]
                                                          # Remove punctuation marks
    token_lz =remove_punc(token_lz)
   # tab= str.maketrans('', '', '\t')
    #token_lz = [word.translate(tab) for word in token_lz]
   # pu = (string.punctuation).replace("'", "")
    #trable = str.maketrans('', '', pu)
    #stripped_words = [word.translate(trable) for word in token_lz]
    #token_lz = [str for str in stripped_words if str]
 
    # Change to lowercase.
    token_lz=[word.lower() for word in token_lz]
    while("" in token_lz) :# removing blank spaces
      token_lz.remove("")
    while(" " in token_lz) :
      token_lz.remove(" ")
  #  for i in token_list:
   #   i= remove_apostrophe(i)

 

    return token_lz

preprocessing('lion stood thoughtfully for a moment')

from collections import defaultdict
import nltk
from nltk.tokenize import RegexpTokenizer

import string
import re
token_file={}       # dictionary that stores the list of the documents for each word
total_doCID=[]      # list containing all the doc ids

# iterating over every file
for fileName in names_file:
  # print(fileName)
  stuff=read_file(fileName)
  try:
    stuff=stuff.lower() # convering the text to lower cases 
    tokenizer=RegexpTokenizer(r'\w+')
    # print(stuff)
    temping1=preprocessing(stuff) # processing the text
    # print(temping1)
    az=len(fileName)
    doc_id=fileName[fileName.rfind('/')+1:az]
    total_doCID.append(doc_id)
    for j in temping1:
        #t_C=0;
        if j not in token_file.keys():
            token_file[j]=[]                # creating new list for new word
            token_file[j].append(doc_id)
            #t_C=t_C+1
        else :
            #t_C=frequency_score.get(j)
            #t_C=t_C+1
            token_file[j].append(doc_id)
  except:
    a = 0
#     print(fileName)
# print(len(token_file))
#pri





total_doCID

# part c) queries
def queries(sent, op, comp):
  ans_set = set()

  # checking if the first word of the query is present in the dataset or not
  if (sent[0] in token_file):
    ans_set = set(token_file[sent[0]])

  # performing the given operations
  for i in range(len(op)):
    op[i] = op[i].strip()
    word2 = [] if sent[i + 1] not in token_file else token_file[sent[i + 1]]
    #print(word2)

    # different operation queries
    if op[i] == 'OR':
          ans_set.update(word2)
          comp = len(ans_set) + len(word2)
    elif op[i] == 'OR NOT':
         # l = total_doc_id - word2
          l = [i for i in total_doCID if i not in word2]
          comp += len(ans_set) + len(word2)
          ans_set.update(l)
    elif op[i] == 'AND':
          l = [v for v in ans_set if v in word2]
          ans_set = set(l)
          comp += len(ans_set) + len(word2)
          #co += min(len(ans_set), len(word2))
    elif op[i] == 'AND NOT': 
          l = [v for v in ans_set if v not in word2]
          ans_set = set(l)
          comp += len(ans_set) + len(total_doCID) - len(word2)
         # co += min(len(ans_set), len(word2))
  return ans_set, comp

# taking inputs and displaying the output for the queries
no_of_queries = int(input())
for i in range(no_of_queries):
  inp_sentence = input()
  inp_sentence = preprocessing(inp_sentence)
  
  print(inp_sentence)
  inp_op = input()
  inp_op = inp_op.strip("[]")
  inp_op = inp_op.split(',')
  
  inp_op = [x.strip(' ') for x in inp_op]
  print(inp_op)
  #print("AAAAAA")
  comp = 0
  output, comp1 = queries(inp_sentence, inp_op, comp)
 # print("ZZZZZZZZ")
  print(output)
  print(len(output))
  print("Number of comparisons are: " )
  print(comp1)

len(set(token_file['charlie']))





