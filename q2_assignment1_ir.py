# -*- coding: utf-8 -*-
"""Q2_ASSIGNMENT1_ir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gs-gr6JnOlWEmC3Ec15BgEQCw9w_x-ig
"""

from google.colab.patches import cv2_imshow
import csv
import cv2
from google.colab import files
!pip install nltk
!pip install pandas
!pip install numpy
!pip install ipython-autotime
from natsort import natsorted
import string
import matplotlib.pyplot as plt
import numpy as np
from google.colab.patches import cv2_imshow
import pandas as pd
from google.colab import files
import numpy as np
import os
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
import nltk
nltk.download('stopwords')
import nltk
import string
import re
import os
import string
import numpy as np
import copy
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import nltk
import os
import pickle

from google.colab import drive
drive.mount('/content/drive')





def read_file(filename):
    with open(filename, 'r', encoding ="ascii", errors ="surrogateescape") as f:
        s = f.read()
    f.close()
    
    return s.lower()
def remove(string_a):
    string_fin= ""
    tokens =string_a.split('\n\n')
    for token in tokens[0:]:
        string_fin=string_fin+ token+" "
    return string_fin.lower()

def remove_punc(final_token):
    trans_tab= str.maketrans('', '', '\t')# Removing punctuations. # Q2 part a part iv)
    final_token = [word1.translate(trans_tab) for word1 in final_token]
    name_punc = (string.punctuation).replace("'", "")
    trans_table = str.maketrans('', '', name_punc)
    wordss= [word.translate(trans_table) for word in final_token]
    final_token = [str for str in wordss if str]
    return final_token



from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
path = "/content/drive/MyDrive/Humor,Hist,Media,Food"
os.chdir(path)
names_file = natsorted(os.listdir(path))

def preprocessing(string_b):
    tokenizer = TweetTokenizer() # Q2 part a part ii)
    final_token = tokenizer.tokenize(string_b)
    stoping_word= set(stopwords.words("english")) # stop words 
    final_token = [word1 for word1 in final_token if word1 not in stoping_word]# removing stop words # Q2 part a part iii)
    
    final_token= remove_punc(final_token) # reomving punctiations
 
    final_token =[word1.lower() for word1 in final_token] #Change to lower_Case # Q2 part a part i)
 
    while("" in final_token) :# Remoibing blank spaces. # Q2 part a part v)
      final_token.remove("")
    while(" " in final_token) :
      final_token.remove(" ")
    return final_token

def return_position(t,positions_1, file_number,positions1):
              positions_1[t] = []
              positions_1[t].append(1)
              positions_1[t].append({})  
              return  [positions1]

# refered from GFG https://www.geeksforgeeks.org/python-positional-index/ 
file_number = 0
fileName=[]
positions_1= {} # dictionary to store the frequencies and the psoitions of the tokens in tthe documents 
for file_name in names_file:
        fileName.append(file_name)
        s= read_file(path+'/' + file_name)
        try:
          tokenS_list = preprocessing(s)

          for positions1, t in enumerate(tokenS_list):
            if t not  in positions_1:
              positions_1[t][1][file_number] = return_position(t,positions_1, file_number,positions1)
            else:

              positions_1[t][0]=1+ positions_1[t][0]
              if file_number not in positions_1[t][1]:
                positions_1[t][1][file_number]=[positions1]
              else:
                positions_1[t][1][file_number].append(positions1)
 
        except:
          print(file_number)         
        file_number += 1









def return_append(a1,l1,t_c,k1,tot_docc):
        if l1==t_c:
                return (a1[k1][0])
def return_doc( az):
  docs_list = [z1 for z1 in az]
  return docs_list



def single_word(q_token,query):
        print("total documents")
        c1=0
        zzzz=[]
        if(query not in positions_1.keys()):
          print("EMPTY")
          return

        for a in positions_1[query][1]:
            print(a)
            zzzz.append(fileName[a])
            c1=c1+1
        print(zzzz)
        print(c1)
        
        return
def return_to_delete(q_token):
    
    return q_token[0]

def answers(query): # queery 
    q_token =(preprocessing(query)) # prpcessed aeury returning tokens
   # print()
   # print("DDDD")
    
    print(q_token)
    if(len(q_token)==0):
      print("EMPTY")

      return

    #print("WEE")
    print(type(q_token))
    if len(q_token)==1: # singel word
        single_word(q_token,query)
        return
    fin_mod=[]
    if(q_token[0] not in positions_1.keys()):
      print("EMPTY")
      return
    word_postings = positions_1[q_token[0]][1]
    for ps in positions_1[q_token[0]][1]:
        for p in positions_1[q_token[0]][1][ps]:
            fin_mod.append((ps, p))
    
    del q_token[0]
    tot_count_docs_match=[] # documetn numberss
    
    tot_count_docs_match1=[] # document names
    k1=0
    while k1< len(fin_mod): # multiple words in query
        zz= fin_mod[k1]
        t_c=0
        pos=zz[1]
        for k in q_token:#For the dictionary of the first token in form of  { doc id, position array}, iterate over all the doc IDs of the first token, and for all positions in that list as start position index and see for the subsequent query tokens. That very doc ID would be the keys,and the next start as the starting postion.

            if(k not in positions_1.keys()):
               print("EMPTY")
               return
            pos=pos+1
            if zz[0] in return_doc(positions_1[k][1]):
                doc_positions ={}
                for posting_value in positions_1[k][1]:
        
                  if posting_value == zz[0]:
                    doc_positions= positions_1[k][1][posting_value]
                    break
                
                
                if  pos not in doc_positions:
                    t_c=1+t_c
                    break
                else:
                    t_c=1+t_c
            if t_c== len(q_token):
                zza=return_append(fin_mod,len(q_token),t_c,k1,tot_count_docs_match)
                tot_count_docs_match.append(zza)
                tot_count_docs_match1.append(fileName[zza])
        k1=k1+1    #With these conditions the docid is added to the lsit.

    print("Total Document Matches:", set(tot_count_docs_match))
    print("NAMES  ", set(tot_count_docs_match1))
    print(len(set(tot_count_docs_match)))

token_a='welcome'

token_a= preprocessing(token_a)
   # print(preprocessed_word[0])
print("Frequency:",positions_1[token_a[0]][0])
print("Postings List:",positions_1[token_a[0]][1])

query = "welcome"
lists = answers(query)

query = "Strange names"
lists = answers(query)

query = "bulletin board"
lists = answers(query)

query = "it is a large mixer BOWL"
lists = answers(query)

fileName[663]

query = "xxdggdhdgk welcome"

lists = answers(query)

